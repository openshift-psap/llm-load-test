# llm-load-test Profile Datasets

The datasets in this directory are generated by the script `contrib/synthetic_datagen/generate_profile.py` which uses a series of distribution profiles and a corpus text to produce normally-distributed datasets.

| Profile | Mean  | StDev |
|---------|-------|-------|
| S       | 128   | 32    |
| M       | 512   | 128   |
| L       | 2048  | 512   |
| XL      | 10240 | 2048  |

Each dataset has the following naming convention:

```
<input profile>I<output profile>O_<distribution>_<tokenizer>.jsonl
```

Some possible use-cases for each dataset:

- `LIMO_normal_granite-3.1-8b-instruct.jsonl`: Best default as it is most similar to `openorca_large_subset_011.jsonl`, our previous default.
- `MILO_normal_granite-3.1-8b-instruct.jsonl`: Best for measuring output throughput with a normal input.
- `LISO_normal_granite-3.1-8b-instruct.jsonl`: Best for short summarization scenarios such as RAG.
- `MIMO_normal_granite-3.1-8b-instruct.jsonl`: Best for testing balanced I/O.
- `SILO_normal_granite-3.1-8b-instruct.jsonl`: Best for measuring peak output throughput.
- `SISO_normal_granite-3.1-8b-instruct.jsonl`: Best for measuring peak request throughput and extra small models.

Example configuration:

``` yaml
output:
  format: "json"
  dir: "./output/"
  file: "output-{concurrency:03d}.json"
dataset:
  file: "datasets/profiles/LIMO_normal_granite-3.1-8b-instruct.jsonl"
  max_queries: 4000
load_options:
  type: constant
  concurrency: [1, 2, 4, 8, 32, 64, 128]
  duration: 300
plugin: "openai_plugin"
plugin_options:
  endpoint: "/v1/completions"
  use_tls: False
  streaming: True
  model_name: "ibm-granite/granite-3.1-8b-instruct"
  host: "http://127.0.0.1:8000"
  authorization: "" # Set if host requires Authorization Token
```
